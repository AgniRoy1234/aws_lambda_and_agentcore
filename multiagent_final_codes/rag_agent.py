import boto3
from opensearchpy import OpenSearch, RequestsHttpConnection
from requests_aws4auth import AWS4Auth
from sentence_transformers import SentenceTransformer
import os 
from google import genai
import requests 

import uuid

from langchain.chat_models import init_chat_model
from langchain.agents import create_agent
from langgraph.store.base import BaseStore

from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage,SystemMessage
from langchain_core.runnables import RunnableConfig
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES

# Import AgentCore runtime and memory integrations
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from langgraph_checkpoint_aws import AgentCoreMemorySaver, AgentCoreMemoryStore
from langchain.agents.middleware import AgentMiddleware, AgentState

from transformers import logging as hf_logging
hf_logging.set_verbosity_error()
hf_logging.disable_progress_bar()

app = BedrockAgentCoreApp()
# AgentCore Memory Configuration

VECTOR_FIELD = "embedding"
TEXT_FIELD = "text"
TOP_K = int(os.getenv("TOP_K"))
HOST = os.getenv("HOST")
INDEX_NAME = os.getenv("INDEX_NAME")
REGION = os.getenv("REGION","ap-south-1")
SERVICE = os.getenv("SERVICE","aoss")
GOOGLE_API_KEY= os.getenv("GOOGLE_API_KEY")
GROQ_API_KEY=os.getenv("GROQ_API_KEY")
STRANDS_URL= os.getenv("STRANDS_URL")

os.environ["AWS_REGION"] = "ap-south-1"
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

client = genai.Client(api_key=GOOGLE_API_KEY)

credentials = boto3.Session().get_credentials()

MODEL_NAME = "all-MiniLM-L6-v2"
model_path = os.path.join(os.environ['LAMBDA_TASK_ROOT'], 'model_cache')

# Load the model from the LOCAL path, not the internet
# This prevents the "Read-only file system" error
model = SentenceTransformer(model_path)

MEMORY_ID = os.getenv("MEMORY_ID")
MEMORY_STRATEGY_ID = os.getenv("MEMORY_STRATEGY_ID")

# Initialize memory components
checkpointer = AgentCoreMemorySaver(memory_id=MEMORY_ID,region_name=REGION)
store = AgentCoreMemoryStore(memory_id=MEMORY_ID,region_name=REGION)

import logging 
import json
import sys 

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    stream=sys.stdout
)

logger = logging.getLogger(__name__)

import random
math_llm = init_chat_model(
    model="openai/gpt-oss-20b",
    model_provider="groq",
    api_key=GROQ_API_KEY
)

poetry_llm = init_chat_model(
    model="openai/gpt-oss-20b",
    model_provider="groq",
    api_key=GROQ_API_KEY
)

orchestrator_llm = init_chat_model(
    model="openai/gpt-oss-20b",
    model_provider="groq",
    api_key=GROQ_API_KEY
)

anime_llm = init_chat_model(
    model="openai/gpt-oss-20b",
    model_provider="groq",
    api_key=GROQ_API_KEY
)

@tool
def call_anime_agent(query: str) -> str:
    ''' 
    Calls the external Anime A2A agent service and returns its generated response.

    Args:
        query (str): Natural language question or instruction related to anime
                     (e.g., recommendations, character details, plot summaries).

    Returns:
        str: The response generated by the Anime agent.
    '''

    payload = {"query": query}

    r = requests.get(STRANDS_URL, json=payload, timeout=90)
    r.raise_for_status()
    data = r.json()
    
    print("This is anime data")
    print(data)

    if "error" in data:
        raise Exception(data["error"])

    return data['response']

# =========================
# Math Tool
# =========================

@tool
def random_math_operation(a: float, b: float) -> str:
    """
    Takes two numbers and performs a random mathematical operation
    among +, -, *, /.
    """
    operations = {
        "addition": lambda x, y: x + y,
        "subtraction": lambda x, y: x - y,
        "multiplication": lambda x, y: x * y,
        "division": lambda x, y: x / y if y != 0 else "undefined (division by zero)"
    }

    op_name = random.choice(list(operations.keys()))
    result = operations[op_name](a, b)

    return f"Operation chosen: {op_name}\nResult: {result}"

# =========================
# Math Agent
# =========================

math_agent = create_agent(
    model=math_llm,
    tools=[random_math_operation],
    system_prompt=(
        "You are a mathematical agent. "
        "When given two numbers, you MUST call the random_math_operation tool."
    )
)


# =========================
# Poetry Agent
# =========================

poetry_agent = create_agent(
    model=poetry_llm,
    tools=[],  # No tools needed
    system_prompt=(
        "You are a poetry agent. "
        "Whenever invoked, return ONLY a two-line poem. "
        "Do not explain anything."
    )
)


# =========================
# Wrap Sub-Agents as Tools
# =========================

@tool
def call_math_agent(query: str) -> str:
    """Calls the math agent with a user query."""
    response = math_agent.invoke({"messages": [{"role": "user", "content": query}]})
    return response["messages"][-1].content


@tool
def call_poetry_agent(query: str) -> str:
    """Calls the poetry agent with a user query."""
    response = poetry_agent.invoke({"messages": [{"role": "user", "content": query}]})
    return response["messages"][-1].content


@tool
def retrieve_documents(user_query: str, k: int = TOP_K):
    """
    Retrieve top-k similar chunks from OpenSearch Serverless
    using cosine similarity (normalized embeddings).
    """
    
    awsauth = AWS4Auth(
    credentials.access_key,
    credentials.secret_key,
    REGION,
    SERVICE,
    session_token=credentials.token,
    )

    opensearch = OpenSearch(
        hosts=[{"host": HOST.replace("https://", ""), "port": 443}],
        http_auth=awsauth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
        timeout=60,
    )

    # IMPORTANT: Must normalize because indexing used normalize_embeddings=True
    query_vector = model.encode(
        user_query,
        convert_to_numpy=True,
        normalize_embeddings=True).tolist()

    search_body = {
        "size": k,
        "query": {
            "knn": {
                "embedding": {
                    "vector": query_vector,
                    "k": k
                }
            }
        }
    }

    response = opensearch.search(
        index=INDEX_NAME,
        body=search_body
    )

    hits = response["hits"]["hits"]

    results = []
    for hit in hits:
        results.append(hit["_source"]["content"])
    
    results = (";\n").join(results)
    
    final_prompt = f"""ANSWER this query with the following context CONTEXT : {results},
                    QUERY : {user_query}"""
    
    response = client.models.generate_content(
            model='gemini-3-flash-preview',  # Specifies which Gemini model to use
            contents=[final_prompt]    )
    
    answer = ""
    
    for candidate in response.candidates:
            if candidate.content.parts:  # Ensure response has content
                for part in candidate.content.parts:
                    answer = part.text 
    
    return answer

tools = [retrieve_documents , call_math_agent, call_poetry_agent, call_anime_agent]

class MemoryMiddleware(AgentMiddleware):
    # Pre-model hook: saves messages and retrieves long-term memories
    def before_model(self, state: AgentState, config: RunnableConfig, *, store: BaseStore):
        """
        Hook that runs before LLM invocation to:
        1. Save the latest human message to long-term memory
        2. Retrieve relevant user preferences and memories
        3. Append memories to the context
        """
        actor_id = config["configurable"]["actor_id"]
        thread_id = config["configurable"]["thread_id"]
        
        logger.info("Pre Model Hook State")
        logger.info(str(state))
        
        # Namespace for this specific session
        messages = state.get("messages", [])
        
        # Save the last human message to long-term memory
        for msg in reversed(messages):
            
            namespace = (
                        "strategies",
                        MEMORY_STRATEGY_ID,
                        "actors",
                        actor_id,
                        "sessions",
                        thread_id
                    )
            try:
                preferences = store.search(
                        namespace, 
                        query=msg.content, 
                        limit=2)
                if preferences:
                    memory_context = "\n".join([
                            f"Memory: {item.value.get('content', '')}" 
                            for item in preferences
                        ])
                        
                    logger.info("memory_context")
                    logger.info(str(memory_context))
                        
                    messages.insert(0,SystemMessage(content=f"Relevant long term memory :\n{memory_context}"))
                    break 
        
                        # You can append this to the messages or use it another way
            except Exception as e:
                logger.info(f"Memory retrieval error:")
                logger.info(str(e))
                break
        
        first_msg = messages[0]
        # --------------------------------------------------
        # ðŸ”¥ Keep last N full user turns (SAFE TRIMMING)
        # --------------------------------------------------
        max_turns = 2  # change as needed
        trimmed = []
        turn_count = 0

        # Walk backwards safely
        for msg in reversed(messages[1:]):  # skip system
            trimmed.insert(0, msg)

            if isinstance(msg, HumanMessage):
                turn_count += 1
                if turn_count >= max_turns:
                    break

        new_messages = [first_msg] + trimmed

        # --------------------------------------------------
        # ðŸ”¥ Hard replace entire state
        # --------------------------------------------------
        return {
            "messages": [
                RemoveMessage(id=REMOVE_ALL_MESSAGES),
                *new_messages
            ]
        }

    # OPTIONAL: Post-model hook to save AI responses
    def after_model(self,state: AgentState, config: RunnableConfig, *, store: BaseStore):
        """
        Hook that runs after LLM invocation to save AI messages to long-term memory
        """
        actor_id = config["configurable"]["actor_id"]
        thread_id = config["configurable"]["thread_id"]
        namespace = (actor_id,thread_id)
        
        logger.info("Post Model Hook State")
        logger.info(state)
        
        messages = state.get("messages", [])
        
        # Save the last AI message
        for msg in reversed(messages):
            store.put(namespace, str(uuid.uuid4()), {"message": msg})
            break
        
        return state


S3_BUCKET = "agentcoreprompts"
S3_KEY = "system_prompt.txt"

def load_system_prompt_from_s3():
    s3 = boto3.client("s3", region_name=REGION)

    response = s3.get_object(
        Bucket=S3_BUCKET,
        Key=S3_KEY
    )

    prompt_text = response["Body"].read().decode("utf-8")
    return prompt_text.strip()

system_prompt = load_system_prompt_from_s3() 

# Create the agent with memory configurations
agent = create_agent(
    model= orchestrator_llm,
    tools=tools,
    checkpointer=checkpointer,
    store=store,
    middleware=[MemoryMiddleware()],
    system_prompt=system_prompt,
)

# AgentCore Entrypoint
@app.entrypoint
def agent_invocation(payload, context):
    """Handler for agent invocation in AgentCore runtime with memory support"""
    logger.info("Received payload:")
    logger.info(payload)
    logger.info("Context:") 
    logger.info(context)
    
    # Extract query from payload
    query = payload.get("prompt", "No prompt found in input")
    
    # Extract or generate actor_id and thread_id
    actor_id = payload.get("actor_id", "default-user")
    thread_id = payload.get("thread_id", payload.get("session_id", "default-session"))
    
    # Configure memory context
    config = {
        "configurable": {
            "thread_id": thread_id,  # Maps to AgentCore session_id
            "actor_id": actor_id     # Maps to AgentCore actor_id
        }
    }
    
    # Invoke the agent with memory
    result = agent.invoke(
        {"messages": [("human", query)]},
        config=config
    )
    
    logger.info("Result:")
    logger.info(result)
    
    # Extract the final answer from the result
    messages = result.get("messages", [])
    answer = messages[-1].content if messages else "No response generated"
    
    # Return the answer
    return {
        "result": answer,
        "actor_id": actor_id,
        "thread_id": thread_id
    }

if __name__ == "__main__":
    app.run()

